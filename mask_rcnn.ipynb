{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maskrcnn\n",
    "\n",
    "A maskrcnn model based on the model here:  \n",
    "https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=at-h4OWK0aoc \n",
    "is used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from matplotlib import colors\n",
    "import torch, torchvision\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/fastai_dev/dev')\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.callback.all import *\n",
    "from local.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  train.csv     train_images.zip\r\n",
      "test_images.zip        train_images  understanding_cloud_organization.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5546) [data/train_images/0a7a247.jpg,data/train_images/2f52d76.jpg,data/train_images/6b272fe.jpg,data/train_images/01eecc1.jpg,data/train_images/f3dad96.jpg,data/train_images/93aafb4.jpg,data/train_images/f157992.jpg,data/train_images/4fa9d86.jpg,data/train_images/c71b0dc.jpg,data/train_images/547ad87.jpg...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = get_image_files('data/train_images/')\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_annotation(fpath):\n",
    "    df = pd.read_csv(fpath)\n",
    "    df['Image'] = df.Image_Label.apply(lambda o: o.split('.')[0])\n",
    "    df['Label'] = df.Image_Label.apply(lambda o: o.split('_')[1].lower())\n",
    "    df.drop('Image_Label', axis=1, inplace=True)\n",
    "    df = df[['Image', 'Label', 'EncodedPixels']]\n",
    "    return df\n",
    "\n",
    "annots = load_train_annotation('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle: str = '', shape: tuple = (1400, 2100)):\n",
    "    '''\n",
    "    Decode rle encoded mask.\n",
    "    \n",
    "    :param mask_rle: run-length as string formatted (start length)\n",
    "    :param shape: (height, width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \n",
    "    Copied from https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')\n",
    "\n",
    "\n",
    "class CloudTypesImage(Tuple):\n",
    "    def show(self, ax=None, figsize=None):\n",
    "        imgid, img, masks = self\n",
    "        if ax is None: _, ax = plt.subplots(figsize=figsize)\n",
    "        ax.imshow(img)\n",
    "        for cloud, m, in masks.items():\n",
    "            if m.sum() == 0: continue\n",
    "            m = np.ma.masked_where(m < 1, m)\n",
    "            ax.imshow(m, alpha=.7, \n",
    "                      cmap=colors.ListedColormap([COLORS[cloud]]))\n",
    "        present_clouds = [cloud for cloud, m in masks.items() if m.sum() > 0]\n",
    "        ax.set_title(f\"{imgid}:{','.join(present_clouds)}\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "            \n",
    "class CloudTypesTfm(Transform):\n",
    "    def __init__(self, items, annots):\n",
    "        self.items, annots = items, annots\n",
    "        \n",
    "    def encodes(self, i):\n",
    "        fn = self.items[i]\n",
    "        img = PILImage.create(fn)\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[annots.Image==imgid]\n",
    "        df.EncodedPixels.fillna(value='', inplace=True)\n",
    "        df.loc[:,'Mask'] = df.EncodedPixels.apply(partial(rle_decode, shape=img.shape))\n",
    "        masks = {o:df[df.Label==o].Mask.values[0] for o in df.Label}\n",
    "        return imgid, img, masks\n",
    "        \n",
    "    def decodes(self, o): return CloudTypesImage(*o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = Category.create(['fish', 'flower', 'gravel', 'sugar'], add_na=True)\n",
    "COLORS = dict(fish='b', flower='r', gravel='y', sugar='c')\n",
    "\n",
    "def get_random_cmap(length):\n",
    "    return colors.ListedColormap([np.random.rand(3,) for _ in range(length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATS('flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskRTargetTfm(Transform):\n",
    "    def __init__(self, items, annots, cats): \n",
    "        self.items, self.annots, self.cats = items, annots, cats\n",
    "        \n",
    "    def encodes(self, i): \n",
    "        fn = self.items[i]\n",
    "        img = PILImage.create(fn)\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[(annots.Image == imgid) & (annots.EncodedPixels.notnull())]\n",
    "        num_objs = len(df)\n",
    "        \n",
    "        boxes, masks, labels = [], [], []\n",
    "        for _, r in df.iterrows(): \n",
    "            mask = rle_decode(r.EncodedPixels, shape=img.shape)\n",
    "            \n",
    "            pos = np.where(mask)\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            \n",
    "            labels.append(self.cats(r.Label))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            masks.append(mask)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "        \n",
    "        target = {}\n",
    "        target['image_id'] = torch.tensor([i])\n",
    "        target['labels'] = labels\n",
    "        target['boxes'] = boxes\n",
    "        target['masks'] = masks\n",
    "        target['area'] = area\n",
    "        target['is_crowd'] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        return target\n",
    "    \n",
    "    def decodes(self, o): return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskr = MaskRTargetTfm(items, annots, CATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': tensor([11]),\n",
       " 'labels': tensor([1, 2, 3, 4]),\n",
       " 'boxes': tensor([[  32.,  253.,  637.,  608.],\n",
       "         [1120.,  740., 1870., 1309.],\n",
       "         [1637., 1123., 2098., 1398.],\n",
       "         [1211.,   24., 2098.,  470.]]),\n",
       " 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 1, 1, 0],\n",
       "          [0, 0, 0,  ..., 1, 1, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " 'area': tensor([214775., 426750., 126775., 395602.]),\n",
       " 'is_crowd': tensor([0, 0, 0, 0])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskr.decode(maskr(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign()\n",
       "  (box_head): TwoMLPHead(\n",
       "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "  )\n",
       "  (mask_roi_pool): MultiScaleRoIAlign()\n",
       "  (mask_head): MaskRCNNHeads(\n",
       "    (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu4): ReLU(inplace=True)\n",
       "  )\n",
       "  (mask_predictor): MaskRCNNPredictor(\n",
       "    (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[  7.7471,  80.5660,  19.2970,  89.3608],\n",
       "          [ 16.5200,  65.4092,  22.5903,  71.5422],\n",
       "          [101.9586,  87.7781, 112.9115,  98.8642],\n",
       "          [ 77.5966,  49.5098,  84.0162,  58.8973],\n",
       "          [  0.0000,   0.0000,   4.2290,   3.9732],\n",
       "          [ 34.5164,  16.7644,  36.3853,  18.6858],\n",
       "          [ 18.0366,  66.1654,  21.6986,  69.6559],\n",
       "          [  9.9811,  81.3504,  19.2893,  89.4333]], grad_fn=<StackBackward>),\n",
       "  'labels': tensor([ 3, 10,  3, 10, 10, 10, 10, 10]),\n",
       "  'scores': tensor([0.1208, 0.1037, 0.0863, 0.0812, 0.0770, 0.0696, 0.0662, 0.0656],\n",
       "         grad_fn=<IndexBackward>),\n",
       "  'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model([torch.randn(3, 100, 120)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
