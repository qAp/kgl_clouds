{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maskrcnn\n",
    "\n",
    "A maskrcnn model based on the model here:  \n",
    "https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=at-h4OWK0aoc \n",
    "is used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from matplotlib import colors\n",
    "import torch, torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/fastai_dev/dev')\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.callback.all import *\n",
    "from local.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  train.csv     train_images.zip\r\n",
      "test_images.zip        train_images  understanding_cloud_organization.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5546) [data/train_images/0a7a247.jpg,data/train_images/2f52d76.jpg,data/train_images/6b272fe.jpg,data/train_images/01eecc1.jpg,data/train_images/f3dad96.jpg,data/train_images/93aafb4.jpg,data/train_images/f157992.jpg,data/train_images/4fa9d86.jpg,data/train_images/c71b0dc.jpg,data/train_images/547ad87.jpg...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = get_image_files('data/train_images/')\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_train_annotation(fpath):\n",
    "    df = pd.read_csv(fpath)\n",
    "    df['Image'] = df.Image_Label.apply(lambda o: o.split('.')[0])\n",
    "    df['Label'] = df.Image_Label.apply(lambda o: o.split('_')[1].lower())\n",
    "    df.drop('Image_Label', axis=1, inplace=True)\n",
    "    df = df[['Image', 'Label', 'EncodedPixels']]\n",
    "    return df\n",
    "\n",
    "annots = load_train_annotation('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle: str = '', shape: tuple = (1400, 2100)):\n",
    "    '''\n",
    "    Decode rle encoded mask.\n",
    "    \n",
    "    :param mask_rle: run-length as string formatted (start length)\n",
    "    :param shape: (height, width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \n",
    "    Copied from https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')\n",
    "\n",
    "\n",
    "class CloudTypesImage(Tuple):\n",
    "    def show(self, ax=None, figsize=None):\n",
    "        imgid, img, masks = self\n",
    "        if ax is None: _, ax = plt.subplots(figsize=figsize)\n",
    "        ax.imshow(img)\n",
    "        for cloud, m, in masks.items():\n",
    "            if m.sum() == 0: continue\n",
    "            m = np.ma.masked_where(m < 1, m)\n",
    "            ax.imshow(m, alpha=.7, \n",
    "                      cmap=colors.ListedColormap([COLORS[cloud]]))\n",
    "        present_clouds = [cloud for cloud, m in masks.items() if m.sum() > 0]\n",
    "        ax.set_title(f\"{imgid}:{','.join(present_clouds)}\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "            \n",
    "class CloudTypesTfm(Transform):\n",
    "    def __init__(self, items, annots):\n",
    "        self.items, annots = items, annots\n",
    "        \n",
    "    def encodes(self, i):\n",
    "        fn = self.items[i]\n",
    "        img = PILImage.create(fn)\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[annots.Image==imgid]\n",
    "        df.EncodedPixels.fillna(value='', inplace=True)\n",
    "        df.loc[:,'Mask'] = df.EncodedPixels.apply(partial(rle_decode, shape=img.shape))\n",
    "        masks = {o:df[df.Label==o].Mask.values[0] for o in df.Label}\n",
    "        return imgid, img, masks\n",
    "        \n",
    "    def decodes(self, o): return CloudTypesImage(*o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CATS = Category.create(['fish', 'flower', 'gravel', 'sugar'], add_na=True)\n",
    "COLORS = dict(fish='b', flower='r', gravel='y', sugar='c')\n",
    "\n",
    "def get_random_cmap(length):\n",
    "    return colors.ListedColormap([np.random.rand(3,) for _ in range(length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATS('flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MaskRTargetTfm(Transform):\n",
    "    def __init__(self, items, annots, cats): \n",
    "        self.items, self.annots, self.cats = items, annots, cats\n",
    "        \n",
    "    def encodes(self, i): \n",
    "        fn = self.items[i]\n",
    "        img = PILImage.create(fn)\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[(annots.Image == imgid) & (annots.EncodedPixels.notnull())]\n",
    "        num_objs = len(df)\n",
    "        \n",
    "        boxes, masks, labels = [], [], []\n",
    "        for _, r in df.iterrows(): \n",
    "            mask = rle_decode(r.EncodedPixels, shape=img.shape)\n",
    "            \n",
    "            pos = np.where(mask)\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            \n",
    "            labels.append(self.cats(r.Label))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            masks.append(mask)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "        \n",
    "        target = {}\n",
    "        target['image_id'] = torch.tensor([i])\n",
    "        target['labels'] = labels\n",
    "        target['boxes'] = boxes\n",
    "        target['masks'] = masks\n",
    "        target['area'] = area\n",
    "        target['is_crowd'] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        return target\n",
    "    \n",
    "    def decodes(self, o): return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, items, annots, cats, transforms=None):\n",
    "        self.items, self.annots, self.cats = items, annots, cats\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.items[idx]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[(annots.Image == imgid) & (annots.EncodedPixels.notnull())]\n",
    "        num_objs = len(df)\n",
    "        \n",
    "        boxes, masks, labels = [], [], []\n",
    "        for _, r in df.iterrows(): \n",
    "            mask = rle_decode(r.EncodedPixels, shape=img.shape)\n",
    "            \n",
    "            pos = np.where(mask)\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            \n",
    "            labels.append(self.cats(r.Label))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            masks.append(mask)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "        \n",
    "        target = {}\n",
    "        target['image_id'] = torch.tensor([idx])\n",
    "        target['labels'] = labels\n",
    "        target['boxes'] = boxes\n",
    "        target['masks'] = masks\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target\n",
    "                                         )\n",
    "        return img, target        \n",
    "        \n",
    "    def __len__(self): return len(self.items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clouds_dataset = CloudDataset(items, annots, CATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=2100x1400 at 0x7F10C1E47110>,\n",
       " {'image_id': tensor([9]),\n",
       "  'labels': tensor([1, 2]),\n",
       "  'boxes': tensor([[  28.,  288.,  515.,  920.],\n",
       "          [ 306.,    7., 2098., 1368.]]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'area': tensor([ 307784., 2438912.]),\n",
       "  'iscrowd': tensor([0, 0])})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clouds_dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign()\n",
       "  (box_head): TwoMLPHead(\n",
       "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "  )\n",
       "  (mask_roi_pool): MultiScaleRoIAlign()\n",
       "  (mask_head): MaskRCNNHeads(\n",
       "    (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu4): ReLU(inplace=True)\n",
       "  )\n",
       "  (mask_predictor): MaskRCNNPredictor(\n",
       "    (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign()\n",
       "  (box_head): TwoMLPHead(\n",
       "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  )\n",
       "  (mask_roi_pool): MultiScaleRoIAlign()\n",
       "  (mask_head): MaskRCNNHeads(\n",
       "    (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu4): ReLU(inplace=True)\n",
       "  )\n",
       "  (mask_predictor): MaskRCNNPredictor(\n",
       "    (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (mask_fcn_logits): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_instance_segmentation_model(len(CATS.vocab)).roi_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision'...\n",
      "remote: Enumerating objects: 5598, done.\u001b[K\n",
      "remote: Total 5598 (delta 0), reused 0 (delta 0), pack-reused 5598\u001b[K\n",
      "Receiving objects: 100% (5598/5598), 9.42 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (3716/3716), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/pytorch/vision.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp references/detection/utils.py ../\n",
    "! cp references/detection/transforms.py ../\n",
    "! cp references/detection/coco_eval.py ../\n",
    "! cp references/detection/engine.py ../\n",
    "! cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = [T.ToTensor()]\n",
    "    if train: pass\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CloudDataset(items, annots, CATS, transforms=get_transform(train=True))\n",
    "dataset_test = CloudDataset(items, annots, CATS, transforms=get_transform(train=False))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = indices[:200]\n",
    "\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = len(CATS.vocab)\n",
    "\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/75]  eta: 0:09:22  lr: 0.000073  loss: 4.0856 (4.0856)  loss_classifier: 1.2874 (1.2874)  loss_box_reg: 0.1296 (0.1296)  loss_mask: 2.3939 (2.3939)  loss_objectness: 0.2328 (0.2328)  loss_rpn_box_reg: 0.0419 (0.0419)  time: 7.5024  data: 1.9595  max mem: 2431\n",
      "Epoch: [0]  [10/75]  eta: 0:01:30  lr: 0.000748  loss: 2.0398 (2.7648)  loss_classifier: 0.5909 (0.6747)  loss_box_reg: 0.1163 (0.0975)  loss_mask: 1.2440 (1.8537)  loss_objectness: 0.0606 (0.0960)  loss_rpn_box_reg: 0.0419 (0.0428)  time: 1.3870  data: 0.1921  max mem: 2708\n",
      "Epoch: [0]  [20/75]  eta: 0:01:00  lr: 0.001422  loss: 1.1838 (1.9518)  loss_classifier: 0.2718 (0.4519)  loss_box_reg: 0.0961 (0.1013)  loss_mask: 0.7014 (1.2742)  loss_objectness: 0.0378 (0.0672)  loss_rpn_box_reg: 0.0475 (0.0571)  time: 0.7776  data: 0.0149  max mem: 2708\n",
      "Epoch: [0]  [30/75]  eta: 0:00:44  lr: 0.002097  loss: 1.0024 (1.6289)  loss_classifier: 0.1914 (0.3674)  loss_box_reg: 0.1111 (0.1095)  loss_mask: 0.5794 (1.0426)  loss_objectness: 0.0283 (0.0556)  loss_rpn_box_reg: 0.0486 (0.0538)  time: 0.7848  data: 0.0143  max mem: 2710\n",
      "Epoch: [0]  [40/75]  eta: 0:00:33  lr: 0.002773  loss: 0.9134 (1.4454)  loss_classifier: 0.1882 (0.3245)  loss_box_reg: 0.1053 (0.1079)  loss_mask: 0.5025 (0.9118)  loss_objectness: 0.0275 (0.0512)  loss_rpn_box_reg: 0.0307 (0.0499)  time: 0.7893  data: 0.0144  max mem: 2710\n",
      "Epoch: [0]  [50/75]  eta: 0:00:22  lr: 0.003448  loss: 0.8786 (1.3361)  loss_classifier: 0.1551 (0.2922)  loss_box_reg: 0.0946 (0.1090)  loss_mask: 0.4991 (0.8361)  loss_objectness: 0.0295 (0.0476)  loss_rpn_box_reg: 0.0336 (0.0511)  time: 0.7898  data: 0.0148  max mem: 2710\n",
      "Epoch: [0]  [60/75]  eta: 0:00:13  lr: 0.004123  loss: 0.8146 (1.2513)  loss_classifier: 0.1507 (0.2697)  loss_box_reg: 0.0982 (0.1088)  loss_mask: 0.4972 (0.7788)  loss_objectness: 0.0295 (0.0446)  loss_rpn_box_reg: 0.0368 (0.0494)  time: 0.7869  data: 0.0144  max mem: 2710\n",
      "Epoch: [0]  [70/75]  eta: 0:00:04  lr: 0.004798  loss: 0.8413 (1.2023)  loss_classifier: 0.1507 (0.2559)  loss_box_reg: 0.1069 (0.1103)  loss_mask: 0.4990 (0.7456)  loss_objectness: 0.0250 (0.0424)  loss_rpn_box_reg: 0.0272 (0.0481)  time: 0.7888  data: 0.0141  max mem: 2710\n",
      "Epoch: [0]  [74/75]  eta: 0:00:00  lr: 0.005000  loss: 0.9166 (1.1911)  loss_classifier: 0.1658 (0.2538)  loss_box_reg: 0.1163 (0.1124)  loss_mask: 0.5082 (0.7334)  loss_objectness: 0.0256 (0.0422)  loss_rpn_box_reg: 0.0368 (0.0493)  time: 0.7939  data: 0.0144  max mem: 2710\n",
      "Epoch: [0] Total time: 0:01:05 (0.8780 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:02:56  model_time: 1.2600 (1.2600)  evaluator_time: 0.9456 (0.9456)  time: 3.5244  data: 1.3060  max mem: 2808\n",
      "Test:  [49/50]  eta: 0:00:01  model_time: 0.9090 (0.9140)  evaluator_time: 0.8304 (0.8192)  time: 1.7917  data: 0.0072  max mem: 2893\n",
      "Test: Total time: 0:01:30 (1.8138 s / it)\n",
      "Averaged stats: model_time: 0.9090 (0.9140)  evaluator_time: 0.8304 (0.8192)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.114\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.108\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.355\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.355\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.083\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.028\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.269\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
