{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maskrcnn\n",
    "\n",
    "A maskrcnn model based on the model here:  \n",
    "https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=at-h4OWK0aoc \n",
    "is used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from matplotlib import colors\n",
    "import torch, torchvision\n",
    "import torch.utils.data\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/fastai_dev/dev')\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.callback.all import *\n",
    "from local.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  train.csv     train_images.zip\r\n",
      "test_images.zip        train_images  understanding_cloud_organization.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5546) [data/train_images/0a7a247.jpg,data/train_images/2f52d76.jpg,data/train_images/6b272fe.jpg,data/train_images/01eecc1.jpg,data/train_images/f3dad96.jpg,data/train_images/93aafb4.jpg,data/train_images/f157992.jpg,data/train_images/4fa9d86.jpg,data/train_images/c71b0dc.jpg,data/train_images/547ad87.jpg...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = get_image_files('data/train_images/')\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_train_annotation(fpath):\n",
    "    df = pd.read_csv(fpath)\n",
    "    df['Image'] = df.Image_Label.apply(lambda o: o.split('.')[0])\n",
    "    df['Label'] = df.Image_Label.apply(lambda o: o.split('_')[1].lower())\n",
    "    df.drop('Image_Label', axis=1, inplace=True)\n",
    "    df = df[['Image', 'Label', 'EncodedPixels']]\n",
    "    return df\n",
    "\n",
    "annots = load_train_annotation('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle: str = '', shape: tuple = (1400, 2100)):\n",
    "    '''\n",
    "    Decode rle encoded mask.\n",
    "    \n",
    "    :param mask_rle: run-length as string formatted (start length)\n",
    "    :param shape: (height, width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \n",
    "    Copied from https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape, order='F')\n",
    "\n",
    "\n",
    "class CloudTypesImage(Tuple):\n",
    "    def show(self, ax=None, figsize=None):\n",
    "        imgid, img, masks = self\n",
    "        if ax is None: _, ax = plt.subplots(figsize=figsize)\n",
    "        ax.imshow(img)\n",
    "        for cloud, m, in masks.items():\n",
    "            if m.sum() == 0: continue\n",
    "            m = np.ma.masked_where(m < 1, m)\n",
    "            ax.imshow(m, alpha=.7, \n",
    "                      cmap=colors.ListedColormap([COLORS[cloud]]))\n",
    "        present_clouds = [cloud for cloud, m in masks.items() if m.sum() > 0]\n",
    "        ax.set_title(f\"{imgid}:{','.join(present_clouds)}\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "            \n",
    "class CloudTypesTfm(Transform):\n",
    "    def __init__(self, items, annots):\n",
    "        self.items, annots = items, annots\n",
    "        \n",
    "    def encodes(self, i):\n",
    "        fn = self.items[i]\n",
    "        img = PILImage.create(fn)\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[annots.Image==imgid]\n",
    "        df.EncodedPixels.fillna(value='', inplace=True)\n",
    "        df.loc[:,'Mask'] = df.EncodedPixels.apply(partial(rle_decode, shape=img.shape))\n",
    "        masks = {o:df[df.Label==o].Mask.values[0] for o in df.Label}\n",
    "        return imgid, img, masks\n",
    "        \n",
    "    def decodes(self, o): return CloudTypesImage(*o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CATS = Category.create(['fish', 'flower', 'gravel', 'sugar'], add_na=True)\n",
    "COLORS = dict(fish='b', flower='r', gravel='y', sugar='c')\n",
    "\n",
    "def get_random_cmap(length):\n",
    "    return colors.ListedColormap([np.random.rand(3,) for _ in range(length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATS('flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MaskRTargetTfm(Transform):\n",
    "    def __init__(self, items, annots, cats): \n",
    "        self.items, self.annots, self.cats = items, annots, cats\n",
    "        \n",
    "    def encodes(self, i): \n",
    "        fn = self.items[i]\n",
    "        img = PILImage.create(fn)\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[(annots.Image == imgid) & (annots.EncodedPixels.notnull())]\n",
    "        num_objs = len(df)\n",
    "        \n",
    "        boxes, masks, labels = [], [], []\n",
    "        for _, r in df.iterrows(): \n",
    "            mask = rle_decode(r.EncodedPixels, shape=img.shape)\n",
    "            \n",
    "            pos = np.where(mask)\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            \n",
    "            labels.append(self.cats(r.Label))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            masks.append(mask)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "        \n",
    "        target = {}\n",
    "        target['image_id'] = torch.tensor([i])\n",
    "        target['labels'] = labels\n",
    "        target['boxes'] = boxes\n",
    "        target['masks'] = masks\n",
    "        target['area'] = area\n",
    "        target['is_crowd'] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        return target\n",
    "    \n",
    "    def decodes(self, o): return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, items, annots, cats, transforms=None):\n",
    "        self.items, self.annots, self.cats = items, annots, cats\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.items[idx]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "        \n",
    "        imgid = fn.stem\n",
    "        df = annots[(annots.Image == imgid) & (annots.EncodedPixels.notnull())]\n",
    "        num_objs = len(df)\n",
    "        \n",
    "        boxes, masks, labels = [], [], []\n",
    "        for _, r in df.iterrows(): \n",
    "            mask = rle_decode(r.EncodedPixels, shape=img.shape)\n",
    "            \n",
    "            pos = np.where(mask)\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            \n",
    "            labels.append(self.cats(r.Label))\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            masks.append(mask)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
    "        \n",
    "        target = {}\n",
    "        target['image_id'] = torch.tensor([idx])\n",
    "        target['labels'] = labels\n",
    "        target['boxes'] = boxes\n",
    "        target['masks'] = masks\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target\n",
    "                                         )\n",
    "        return img, target        \n",
    "        \n",
    "    def __len__(self): return len(self.items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clouds_dataset = CloudDataset(items, annots, CATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<local.vision.core.PILImage image mode=RGB size=2100x1400 at 0x7FEC4B489B90>,\n",
       " {'image_id': tensor([9]),\n",
       "  'labels': tensor([1, 2]),\n",
       "  'boxes': tensor([[  28.,  288.,  515.,  920.],\n",
       "          [ 306.,    7., 2098., 1368.]]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'area': tensor([ 307784., 2438912.]),\n",
       "  'is_crowd': tensor([0, 0])})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clouds_dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign()\n",
       "  (box_head): TwoMLPHead(\n",
       "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "  )\n",
       "  (mask_roi_pool): MultiScaleRoIAlign()\n",
       "  (mask_head): MaskRCNNHeads(\n",
       "    (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu4): ReLU(inplace=True)\n",
       "  )\n",
       "  (mask_predictor): MaskRCNNPredictor(\n",
       "    (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roi_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoIHeads(\n",
       "  (box_roi_pool): MultiScaleRoIAlign()\n",
       "  (box_head): TwoMLPHead(\n",
       "    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (box_predictor): FastRCNNPredictor(\n",
       "    (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "    (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  )\n",
       "  (mask_roi_pool): MultiScaleRoIAlign()\n",
       "  (mask_head): MaskRCNNHeads(\n",
       "    (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu4): ReLU(inplace=True)\n",
       "  )\n",
       "  (mask_predictor): MaskRCNNPredictor(\n",
       "    (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (mask_fcn_logits): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_instance_segmentation_model(len(CATS.vocab)).roi_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision'...\n",
      "remote: Enumerating objects: 5598, done.\u001b[K\n",
      "remote: Total 5598 (delta 0), reused 0 (delta 0), pack-reused 5598\u001b[K\n",
      "Receiving objects: 100% (5598/5598), 9.42 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (3716/3716), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/pytorch/vision.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.chdir('vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "! cp references/detection/utils.py ../\n",
    "! cp references/detection/transforms.py ../\n",
    "! cp references/detection/coco_eval.py ../\n",
    "! cp references/detection/engine.py ../\n",
    "! cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = [T.ToTensor()]\n",
    "    if train: pass\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CloudDataset(items, annots, CATS, transforms=get_transform(train=True))\n",
    "dataset_test = CloudDataset(items, annots, CATS, transforms=get_transform(train=False))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = indices[:200]\n",
    "\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = len(CATS.vocab)\n",
    "\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/75]  eta: 0:03:10  lr: 0.000073  loss: 4.0856 (4.0856)  loss_classifier: 1.2874 (1.2874)  loss_box_reg: 0.1296 (0.1296)  loss_mask: 2.3939 (2.3939)  loss_objectness: 0.2328 (0.2328)  loss_rpn_box_reg: 0.0419 (0.0419)  time: 2.5389  data: 1.7019  max mem: 5210\n",
      "Epoch: [0]  [10/75]  eta: 0:01:01  lr: 0.000748  loss: 2.0427 (2.7665)  loss_classifier: 0.5894 (0.6758)  loss_box_reg: 0.1166 (0.0978)  loss_mask: 1.2456 (1.8540)  loss_objectness: 0.0606 (0.0960)  loss_rpn_box_reg: 0.0419 (0.0428)  time: 0.9438  data: 0.1717  max mem: 5210\n",
      "Epoch: [0]  [20/75]  eta: 0:00:47  lr: 0.001422  loss: 1.1770 (1.9503)  loss_classifier: 0.2613 (0.4512)  loss_box_reg: 0.1009 (0.1011)  loss_mask: 0.7020 (1.2736)  loss_objectness: 0.0379 (0.0673)  loss_rpn_box_reg: 0.0474 (0.0571)  time: 0.7812  data: 0.0158  max mem: 5210\n",
      "Epoch: [0]  [30/75]  eta: 0:00:37  lr: 0.002097  loss: 0.9944 (1.6273)  loss_classifier: 0.1835 (0.3655)  loss_box_reg: 0.1187 (0.1082)  loss_mask: 0.6007 (1.0442)  loss_objectness: 0.0282 (0.0556)  loss_rpn_box_reg: 0.0486 (0.0537)  time: 0.7852  data: 0.0128  max mem: 5210\n",
      "Epoch: [0]  [40/75]  eta: 0:00:29  lr: 0.002773  loss: 0.9367 (1.4483)  loss_classifier: 0.1775 (0.3234)  loss_box_reg: 0.1144 (0.1076)  loss_mask: 0.5103 (0.9157)  loss_objectness: 0.0271 (0.0517)  loss_rpn_box_reg: 0.0311 (0.0499)  time: 0.7926  data: 0.0133  max mem: 5210\n",
      "Epoch: [0]  [50/75]  eta: 0:00:20  lr: 0.003448  loss: 0.8756 (1.3420)  loss_classifier: 0.1608 (0.2925)  loss_box_reg: 0.1008 (0.1102)  loss_mask: 0.5026 (0.8401)  loss_objectness: 0.0315 (0.0480)  loss_rpn_box_reg: 0.0345 (0.0512)  time: 0.7939  data: 0.0137  max mem: 5210\n",
      "Epoch: [0]  [60/75]  eta: 0:00:12  lr: 0.004123  loss: 0.8466 (1.2551)  loss_classifier: 0.1563 (0.2689)  loss_box_reg: 0.1132 (0.1098)  loss_mask: 0.4929 (0.7823)  loss_objectness: 0.0315 (0.0448)  loss_rpn_box_reg: 0.0354 (0.0493)  time: 0.7903  data: 0.0133  max mem: 5210\n",
      "Epoch: [0]  [70/75]  eta: 0:00:04  lr: 0.004798  loss: 0.8680 (1.2100)  loss_classifier: 0.1544 (0.2552)  loss_box_reg: 0.1189 (0.1114)  loss_mask: 0.5269 (0.7529)  loss_objectness: 0.0277 (0.0425)  loss_rpn_box_reg: 0.0282 (0.0481)  time: 0.7891  data: 0.0129  max mem: 5210\n",
      "Epoch: [0]  [74/75]  eta: 0:00:00  lr: 0.005000  loss: 0.9195 (1.1989)  loss_classifier: 0.1693 (0.2533)  loss_box_reg: 0.1306 (0.1139)  loss_mask: 0.5269 (0.7401)  loss_objectness: 0.0269 (0.0421)  loss_rpn_box_reg: 0.0354 (0.0495)  time: 0.7915  data: 0.0127  max mem: 5210\n",
      "Epoch: [0] Total time: 0:01:01 (0.8138 s / it)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f94e7a0be424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/fastai_dev/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kgl_clouds/engine.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Test:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0miou_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_iou_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mcoco_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kgl_clouds/coco_utils.py\u001b[0m in \u001b[0;36mget_coco_api_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kgl_clouds/coco_utils.py\u001b[0m in \u001b[0;36mconvert_to_coco_api\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# find better way to get target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# targets = ds.get_annotations(img_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mimg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-bbea1c7b29fd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrle_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncodedPixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-fd4d13a6edd1>\u001b[0m in \u001b[0;36mrle_decode\u001b[0;34m(mask_rle, shape)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mCopied\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaggle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0martgor\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0musing\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconvenient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     '''\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_rle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstarts\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
